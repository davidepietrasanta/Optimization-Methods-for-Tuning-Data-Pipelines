{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Meta-Learner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First check to have a directory like this:\n",
        "\n",
        "```text\n",
        "lib\n",
        "├── data                        # Store data\n",
        "│   ├─ dataset                  # Store datasets\n",
        "│   ├─ metafeatures             # Store metafeatures\n",
        "│   └─ model                    # Store trained ML models\n",
        "├── images                      # Images for presentations, README, etc.\n",
        "├── other                       # Script or Notebook related to the thesis or to the plots\n",
        "├── src                         # Actual code\n",
        "│   ├─ test                     # Test code\n",
        "│   ├─ utils                    # General utility code\n",
        "│   ├─ exceptions.py            # To handle custom exceptions\n",
        "|   └─ config.py                # Common knowledge for the project\n",
        "|── main.py\n",
        "|── requirements.txt\n",
        "|── setup.py\n",
        "|── test.py                     # To test all\n",
        "└── Tutorial.ipynb              # Simple notebook tutorial\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n",
        "To train the meta-learner we first need the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "from src.config import DATASET_FOLDER \n",
        "from src.utils.data_preparation import data_preparation \n",
        "\n",
        "# Just a directory where you've stored your CSV datasets.\n",
        "prova = join(DATASET_FOLDER, 'prova') \n",
        "\n",
        "data_preparation(\n",
        "    data_path=prova,\n",
        "    data_selection = False,\n",
        "    data_preprocess = True,\n",
        "    metafeatures_extraction = True,\n",
        "    model_training = True,\n",
        "    quotient=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 'data preparation' function can perform multiple functions: \n",
        "* **Data download**: downloads the datasets. If you already have a dataset you can disable it by setting `data_selection = False`\n",
        "* **Data preprocessing**: performs all preprocessing of all datasets. If you have already done it you can disable it by setting `data_preprocess = False`\n",
        "* **Metafeatures Extraction**: extract metafeatures from all datasets, preprocessed and not. If you have already done it you can disable it by setting `metafeatures_extraction = False`\n",
        "* **Models Training**: train all models on all datasets, preprocessed and not. If you have already done it you can disable it by setting `model_training = False`\n",
        "\n",
        "* **Quotient** is used to regulate the delta. If it's False the difference between the metrics is done by a quotient, else by a subtraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to train on the delta of the performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "from src.utils.metalearner import train_metalearner\n",
        "from src.config import METAFEATURES_FOLDER\n",
        "\n",
        "\n",
        "delta_path = join(METAFEATURES_FOLDER, \"delta.csv\")\n",
        "\n",
        "train_metalearner(\n",
        "    metafeatures_path = delta_path,\n",
        "    algorithm='random_forest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to train on the raw data and than compute the difference (delta) after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "from src.utils.data_preparation import choose_performance_from_metafeatures\n",
        "from src.utils.metalearner import train_metalearner\n",
        "from src.config import METAFEATURES_FOLDER\n",
        "\n",
        "metafeatures_path = join(METAFEATURES_FOLDER, \"metafeatures.csv\")\n",
        "\n",
        "choose_performance_from_metafeatures(\n",
        "    metafeatures_path = metafeatures_path,\n",
        "    metric='f1_score',\n",
        "    copy_name='new_metafeatures.csv')\n",
        "\n",
        "new_metafeatures_path = join(METAFEATURES_FOLDER, \"new_metafeatures.csv\")\n",
        "\n",
        "train_metalearner(\n",
        "    metafeatures_path = new_metafeatures_path,\n",
        "    algorithm='random_forest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check if it's better to use delta_metafeatures or metafeatures we can use `delta_or_metafeatures`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.data_preparation import delta_or_metafeatures\n",
        "\n",
        "delta_path = join(METAFEATURES_FOLDER, \"delta.csv\")\n",
        "metafeatures_path = join(METAFEATURES_FOLDER, \"metafeatures.csv\")\n",
        "delta_or_metafeatures(delta_path=delta_path, metafeatures_path=metafeatures_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction\n",
        "\n",
        "If you want to estimate the improvement rate of a dataset after preprocessing you have to use the function `predicted_improvement`.\n",
        "\n",
        "The estimate takes into account the machine learning model considered. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.preprocessing_improvement import predicted_improvement\n",
        "from src.config import METAFEATURES_MODEL_FOLDER\n",
        "\n",
        "some_dataset = join(\n",
        "            DATASET_FOLDER,\n",
        "            join('Test', 'wine-quality-white.csv')\n",
        "            )\n",
        "\n",
        "predicted_improvement(\n",
        "    dataset_path= some_dataset,\n",
        "    preprocessing = 'pca',\n",
        "    algorithm = 'svm',\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_gaussian_process.joblib')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most of the time taken by the prediction is due to the preprocessing time of the dataset.\n",
        "\n",
        "If the preprocessing has already been carried out, this can be indicated by means of the variable `preprocessing_path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.config import TEST_FOLDER\n",
        "test_dataset_path = join(TEST_FOLDER, 'data')\n",
        "\n",
        "predicted_improvement(\n",
        "    dataset_path= some_dataset,\n",
        "    preprocessing_path = join(test_dataset_path, 'pca', 'kc1.csv'),\n",
        "    algorithm = 'svm',\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_random_forest.joblib')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brute force\n",
        "\n",
        "If you want to search for the best for brute force you can use the function `one_step_bruteforce`.\n",
        "\n",
        "This function returns a dictionary with the preprocessing used and the estimated delta.\n",
        "\n",
        "If you want to have the best preprocessing, without having the full list of estimates, you have to use `best_one_step_bruteforce`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.preprocessing_improvement import one_step_bruteforce\n",
        "\n",
        "results = one_step_bruteforce(\n",
        "    dataset_path= some_dataset,\n",
        "    algorithm = 'svm',\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_random_forest.joblib')\n",
        ")\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.preprocessing_improvement import best_one_step_bruteforce\n",
        "\n",
        "best = best_one_step_bruteforce(\n",
        "    dataset_path= some_dataset,\n",
        "    algorithm = 'svm',\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_random_forest.joblib')\n",
        ")\n",
        "\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example of use\n",
        "\n",
        "Suppose I want to use this estimator to calculate a reasonable preprocessing pipeline.\n",
        "\n",
        "I want to consider only `PCA`, `Standard Scaler` and `Feature Agglomeration` as methods.\n",
        "\n",
        "I want the Standard Scaler to be run first and a step may or may not exist. \n",
        "\n",
        "In all, I have 9 possible pipelines to test:\n",
        "\n",
        "1) SS \n",
        "2) SS -> PCA\n",
        "3) SS -> PCA -> FA\n",
        "4) SS -> FA\n",
        "5) SS -> FA -> PCA\n",
        "6) PCA\n",
        "7) PCA -> FA\n",
        "8) FA\n",
        "9) FA -> PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.preprocessing_improvement import pipeline_experiments, max_in_dict\n",
        "\n",
        "list_of_experiments = [\n",
        "    ['standard_scaler'],\n",
        "    ['standard_scaler', 'pca'],\n",
        "    ['standard_scaler', 'pca', 'feature_agglomeration'],\n",
        "    ['standard_scaler', 'feature_agglomeration'],\n",
        "    ['standard_scaler', 'feature_agglomeration', 'pca'],\n",
        "    ['pca'],\n",
        "    ['pca', 'feature_agglomeration'],\n",
        "    ['feature_agglomeration'],\n",
        "    ['feature_agglomeration', 'pca'],\n",
        "]\n",
        "\n",
        "experiments = pipeline_experiments(\n",
        "    dataset_path = some_dataset,\n",
        "    algorithm = 'svm',\n",
        "    list_of_experiments = list_of_experiments,\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_random_forest.joblib')\n",
        "    )\n",
        "\n",
        "print(experiments)\n",
        "\n",
        "[key, value] = max_in_dict(results)\n",
        "print(f\"The best experiment is {key}, with {value} estimated improvement.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to run a single experiment you can either use `pipeline_experiments` or use `preprocessing_experiment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.metafeatures_extraction import metafeature\n",
        "from src.utils.preprocessing_improvement import preprocessing_experiment\n",
        "\n",
        "data_metafeatures = metafeature(some_dataset)\n",
        "result = preprocessing_experiment(\n",
        "    dataset_path = some_dataset,\n",
        "    algorithm = 'svm',\n",
        "    experiment = list_of_experiments[2],\n",
        "    data_metafeatures = data_metafeatures,\n",
        "    metalearner_path = join(METAFEATURES_MODEL_FOLDER, 'metalearner_random_forest.joblib')\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
