"""
    Meta-learner module.
    Used to:\n
        - Create the train/test data\n
        - Train the meta-learner\n
"""
from os.path import join, exists
from os import makedirs
import logging
from joblib import dump
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ConstantKernel
from sklearn.gaussian_process.kernels import RBF
from skopt import BayesSearchCV
from src.config import METAFEATURES_MODEL_FOLDER
from src.config import LIST_OF_ML_MODELS_FOR_METALEARNING
from src.config import SEED_VALUE, TEST_SIZE, SEARCH_SPACE
from src.exceptions import CustomValueError
from .machine_learning_algorithms import prediction_metrics
from .preprocessing_methods import categorical_string_to_number,categorical_algorithm_to_number

def train_metalearner(
    metafeatures_path:str,
    algorithm:str ='random_forest',
    save_path:str = METAFEATURES_MODEL_FOLDER,
    tuning:bool = True):
    """
        Given a dataset and a model it train
         the meta-learner.

        :param algorithm: A machine learning model/algorithm.
         It should be in
            ["knn",
            "random_forest",
            "gaussian_process"]
        :param metafeatures_path: Path to the dataset, should be a CSV file
        generated by 'data_preparation'.
        :param save_path: Where to save the trained model.

        :return: A trained meta-learning and its performances.
    """
    logging.info("Training meta-learner...")

    if algorithm not in LIST_OF_ML_MODELS_FOR_METALEARNING:
        raise CustomValueError(list_name='ml_models_for_metalearning', input_value=algorithm)

    metafeatures = pd.read_csv(metafeatures_path)
    metafeatures = categorical_algorithm_to_number(metafeatures)
    metafeatures = categorical_string_to_number(metafeatures)

    # Split train and test
    logging.info("Splitting train and test...")
    [train, test] = split_train_test(metafeatures, group_name='dataset_name')

    # Drop dataset_name
    train = train.drop(["dataset_name", "preprocessing"], axis=1)
    test = test.drop(["dataset_name", "preprocessing"], axis=1)

    train_y = train["performance"].to_numpy()
    train_x = train.drop(["performance"], axis=1).to_numpy()

    test_y = test["performance"].to_numpy()
    test_x = test.drop(["performance"], axis=1).to_numpy()

    # Train
    logging.info("Training '%s'...", algorithm)
    ml_model = _train(algorithm, train_x, train_y, tuning)
    logging.info("Model trained.")

    # Save
    logging.info("Saving the model...('%s')", algorithm)
    file_path = join(save_path, 'metalearner_' + algorithm + '.joblib')

    if not exists(save_path):
        makedirs(save_path)

    dump(ml_model, file_path)

    # Performance
    performances = prediction_metrics(ml_model, test_x, test_y, metrics = None, regression=True)
    logging.info("Performances: %s", performances)
    return [ml_model, performances]

def _train(algorithm:str, train_x, train_y, tuning):

    if algorithm not in LIST_OF_ML_MODELS_FOR_METALEARNING:
        raise CustomValueError(list_name='ml_models_for_metalearning', input_value=algorithm)

    if algorithm == 'knn':
        n_classes = len( set(train_y) )
        model = knn_regression(
            train_x,
            train_y,
            n_classes,
            tuning)
    elif algorithm == 'random_forest':
        model = random_forest_regression(
            train_x,
            train_y,
            tuning)
    elif algorithm == 'gaussian_process':
        model = gaussian_process(
            train_x,
            train_y,
            tuning)

    return model

def knn_regression(
    x_train,
    y_train,
    n_neighbors:int,
    tuning:bool
    ):
    """
        Given X and y return a trained K-Neighbors model.

        :param X: Input variables
        :param y: Label or Target value
        :param n_neighbors: Number of neighbors to consider

        :return: A trained model.
    """
    model = KNeighborsRegressor(n_neighbors = n_neighbors)

    if tuning:
        search_space = SEARCH_SPACE['knn']
        best_param = hyper_parameters_optimization(
            model,
            x_train,
            y_train,
            search_space)

        # Set best param to the model
        model.set_params(**best_param)

    model.fit(x_train, y_train)
    return model

def random_forest_regression(
    x_train,
    y_train,
    tuning:bool
    ):
    """
        Given X and y return a trained Random Forest model.

        :param X: Input variables
        :param y: Label or Target value

        :return: A trained model.
    """
    model = RandomForestRegressor(random_state=SEED_VALUE)

    if tuning:
        search_space = SEARCH_SPACE['random_forest']
        best_param = hyper_parameters_optimization(
            model,
            x_train,
            y_train,
            search_space)

        # Set best param to the model
        model.set_params(**best_param)

    model.fit(x_train, y_train)
    return model

def gaussian_process(
    x_train,
    y_train,
    tuning:bool
    ):
    """
        Given X and y return a trained Gaussian Process model.

        :param X: Input variables
        :param y: Label or Target value

        :return: A trained model.
    """
    # Note that the kernel hyperparameters are optimized
    # during fitting unless the bounds are marked as “fixed”.
    kernel = ConstantKernel(
        1.0,
        #constant_value_bounds="fixed"
        )*RBF(
            1.0,
            #length_scale_bounds="fixed"
            )

    model = GaussianProcessRegressor(
        kernel = kernel,
        n_restarts_optimizer = 5,
        random_state=SEED_VALUE)

    if tuning:
        search_space = SEARCH_SPACE['gaussian_process']
        best_param = hyper_parameters_optimization(
            model,
            x_train,
            y_train,
            search_space)

        # Set best param to the model
        model.set_params(**best_param)

    model.fit(x_train, y_train)
    return model

def split_train_test(
    dataframe:pd.DataFrame,
    group_name:str,
    test_size:float = TEST_SIZE,
    random_state:int = SEED_VALUE):
    """
        Split a dataframe into train and test keeping
         in the same split items with the same group_name.

        :param dataframe: Dataframe to split.
        :param group_name: Label of the dataframe you want
         to consider to keep the same items in the same split.
        :param test_size: Represent the proportion
        of the dataset to include in the test split.
        :param random_state: Controls the shuffling applied
         to the data before applying the split.

        :return: Return train and test [train, test]
    """

    splitter = GroupShuffleSplit(test_size=test_size, n_splits=2, random_state=random_state)
    split = splitter.split(dataframe, groups=dataframe[group_name])
    train_inds, test_inds = next(split)

    train = dataframe.iloc[train_inds]
    test = dataframe.iloc[test_inds]

    return [train, test]

def hyper_parameters_optimization(model, x_train, y_train, search_space):
    """
        Function to do the hyper-parameters optimization of a given
         sklearn machine learning model.
         To do so it uses the Bayesian Optimization method.

        :param model: sklearn estimator you want to tune.
        :param search_space: Search space you want to consider.

        :return: Best param for the model.
    """
    logging.info("Tuning the hyper-parameters of the model...")

    # Define the search
    search = BayesSearchCV(
        estimator=model,
        search_spaces=search_space,
        n_iter = 100, # [50, 100, 200]
        n_jobs=4,
        random_state = SEED_VALUE
        )

    search.fit(x_train, y_train)

    logging.info("Model best parameters are: %s", str(dict(search.best_params_)))
    logging.info("Model tuned:")

    return search.best_params_
